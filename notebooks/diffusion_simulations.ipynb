{
 "metadata": {
  "name": "",
  "signature": "sha256:9f690a55538a742ef8dc6c791e1fba1c6ee76e448442719740aff5105d0d2033"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "from functools import partial\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "from numpy import linalg as LA\n",
      "\n",
      "from hypergraph import generators\n",
      "from hypergraph.analytical import prediction\n",
      "from hypergraph.diffusion_engine import DiffusionEngine\n",
      "from hypergraph import utils\n",
      "from hypergraph.markov_diffusion import create_markov_matrix_model_nodes\n",
      "from hypergraph.markov_diffusion import create_markov_matrix_model_hyper_edges\n",
      "\n",
      "\n",
      "# Define model's definitions\n",
      "ALL_MODELS = {\n",
      "    \"node\": {\n",
      "        \"analytical\": partial(prediction, model='hypergraph_nodes'),\n",
      "        \"numerical\": create_markov_matrix_model_nodes,\n",
      "        \"name\": \"node\",\n",
      "    },\n",
      "    \"hyperedges\": {\n",
      "        \"analytical\": partial(prediction, model='hypergraph_edges'),\n",
      "        \"numerical\": create_markov_matrix_model_hyper_edges,\n",
      "        \"name\": \"hyperedges\",\n",
      "    }\n",
      "}\n",
      "\n",
      "# Constants for atomistic simulation\n",
      "t_max = 100000\n",
      "number_of_walkers = 1\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "from hypergraph.hypergraph_models import HyperGraph\n",
      "from hypergraph.generators import generic_hypergraph"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TODO:\n",
      "How I would like it to work:\n",
      "\n",
      " - generate random hypergraphs of given properties (another way of generating hypergraphs?)\n",
      " - have function which evaluates model so that, stationary distribution is available\n",
      " - saves hypergraph and results from three? (what about pykov?) ways of computing stationary distributions\n",
      "    to later compare them\n",
      " - another function for comparing results later with nice graphs and pictures!\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "import pykov\n",
      "import os\n",
      "\n",
      "\n",
      "def generate_hypergraph(generator_function, hypergraph_properties):\n",
      "    HG = generator_function(**hypergraph_properties)\n",
      "    return HG\n",
      "\n",
      "\n",
      "def transition_matrix_to_pykov_chain(matrix):\n",
      "    chain = pykov.Chain()\n",
      "    \n",
      "    for i, row in enumerate(matrix):\n",
      "        for j, column in enumerate(row):\n",
      "            chain[(i, j)] = column\n",
      "    return chain"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# for nodes' model\n",
      "\n",
      "def compute_atomistic_results_nodes(HG):\n",
      "    markov_matrix = create_markov_matrix_model_nodes(HG)\n",
      "    t_per_walker = int(t_max / number_of_walkers)\n",
      "    engine = DiffusionEngine(markov_matrix, t_per_walker=t_per_walker)\n",
      "    frequencies, states = engine.simulate(t_max)\n",
      "    connected_elements = [freq[0] + 1 for freq in frequencies]\n",
      "    missing_nodes = set(HG.nodes()) - set(connected_elements)\n",
      "    for missing_node in missing_nodes:\n",
      "        frequencies.append((missing_node, 0))\n",
      "\n",
      "    frequencies = [(node, frequency) for node, frequency in frequencies]\n",
      "    frequencies.sort(key=lambda x: x[0])\n",
      "    xs, ys = zip(*frequencies)\n",
      "\n",
      "    ys = np.array(ys, dtype='float')\n",
      "    ys /= sum(ys)\n",
      "    return xs, ys\n",
      "\n",
      "\n",
      "def compute_matrix_power_results_nodes(HG):\n",
      "    markov_matrix = create_markov_matrix_model_nodes(HG)\n",
      "\n",
      "    freqs_matrix = LA.matrix_power(markov_matrix, 40)[0]\n",
      "    ys = freqs_matrix\n",
      "    xs = range(len(ys))\n",
      "    return xs, ys\n",
      "\n",
      "\n",
      "def compute_pykov_results_nodes(HG):\n",
      "    mm = create_markov_matrix_model_nodes(HG)\n",
      "    chain = transition_matrix_to_pykov_chain(mm)\n",
      "    chain_transposed = pykov.Chain(chain)\n",
      "    xs, ys = zip(*chain_transposed.steady().items())\n",
      "    return xs, ys\n",
      "\n",
      "\n",
      "def compute_analytical_prediction_nodes(HG):\n",
      "    ys = prediction(model='hypergraph_nodes', graph=HG)\n",
      "    xs = range(len(ys))\n",
      "    return xs, ys\n",
      "\n",
      "\n",
      "# for hyper edges' model\n",
      "def compute_atomistic_results_edges(HG):\n",
      "    markov_matrix = create_markov_matrix_model_hyper_edges(HG)\n",
      "    t_per_walker = int(t_max / number_of_walkers)\n",
      "    engine = DiffusionEngine(markov_matrix, t_per_walker=t_per_walker)\n",
      "\n",
      "    frequencies, states = engine.simulate(t_max)\n",
      "\n",
      "    frequencies = [(node, frequency) for node, frequency in frequencies]\n",
      "    frequencies.sort(key=lambda x: x[0])\n",
      "    xs, ys = zip(*frequencies)\n",
      "\n",
      "    ys = np.array(ys, dtype='float')\n",
      "    ys /= sum(ys)\n",
      "    return xs, ys\n",
      "\n",
      "\n",
      "def compute_matrix_power_results_edges(HG):\n",
      "    markov_matrix = create_markov_matrix_model_hyper_edges(HG)\n",
      "\n",
      "    freqs_matrix = LA.matrix_power(markov_matrix, 40)[0]\n",
      "    ys = freqs_matrix\n",
      "    xs = range(len(ys))\n",
      "    return xs, ys\n",
      "\n",
      "\n",
      "def compute_pykov_results_edges(HG):\n",
      "    mm = create_markov_matrix_model_hyper_edges(HG)\n",
      "    chain = transition_matrix_to_pykov_chain(mm)\n",
      "    chain_transposed = pykov.Chain(chain)\n",
      "    xs, ys = zip(*chain_transposed.steady().items())\n",
      "    return xs, ys\n",
      "\n",
      "\n",
      "def compute_analytical_prediction_edges(HG):\n",
      "    ys = prediction(model='hypergraph_edges', graph=HG)\n",
      "    xs = range(len(ys))\n",
      "    return xs, ys\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "def compute_stationary_distributions(HG, name_to_computation_functions_mapping):\n",
      "    results = {}\n",
      "    for name, computation_function in name_to_computation_functions_mapping.items():\n",
      "        xs, pies = computation_function(HG)\n",
      "        results[name] = pies\n",
      "    return results\n",
      "\n",
      "\n",
      "def serialize(HG):\n",
      "    edges = [list(edge) for edge in HG.hyper_edges()]\n",
      "    return json.dumps(edges)\n",
      "\n",
      "\n",
      "def save_result_distribution(filename, result_distribution):\n",
      "    with open(filename, 'w') as f:\n",
      "        for value in result_distribution:\n",
      "            f.write(\"%s\\n\" % value)\n",
      "\n",
      "\n",
      "def save_hypergraph_values(filename, hg_description):\n",
      "    with open(filename, 'w') as f:\n",
      "        f.write(hg_description)\n",
      "\n",
      "\n",
      "def save_results_to_files(HG, results, counter, directory_name=None):\n",
      "    base_filename = '%s_{name}.csv' % counter\n",
      "    if directory_name:\n",
      "        if not os.path.exists(directory_name):\n",
      "            os.mkdir(directory_name)\n",
      "        base_filename = directory_name + '/' + base_filename\n",
      "    \n",
      "    for name, result_distribution in results.items():\n",
      "        filename = base_filename.format(name=name)\n",
      "        save_result_distribution(filename, result_distribution)\n",
      "    \n",
      "    hg_description = serialize(HG)\n",
      "    filename = base_filename.format(name='hypergraph')\n",
      "    save_hypergraph_values(filename, hg_description)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nodes_mapping = {\n",
      "    'analytical_nodes': compute_analytical_prediction_nodes,\n",
      "    'atomistic_nodes': compute_atomistic_results_nodes,\n",
      "    'matrix_power_nodes': compute_matrix_power_results_nodes,\n",
      "    'pykov_nodes': compute_pykov_results_nodes,\n",
      "}\n",
      "\n",
      "edges_mapping = {\n",
      "    'analytical_edges': compute_analytical_prediction_edges,\n",
      "    'atomistic_edges': compute_atomistic_results_edges,\n",
      "    'matrix_power_edges': compute_matrix_power_results_edges,\n",
      "    'pykov_edges': compute_pykov_results_edges,\n",
      "}\n",
      "\n",
      "def execute_pipeline(generator_function, hypergraph_properties, directory_name, name_to_computation_functions_mapping, n=10):    \n",
      "\n",
      "    for counter in range(n):\n",
      "        HG = generate_hypergraph(generator_function, hypergraph_properties)\n",
      "        results = compute_stationary_distributions(HG, name_to_computation_functions_mapping)\n",
      "        save_results_to_files(HG, results, counter, directory_name=directory_name)\n",
      "        print(\"%s/%s\" % (counter + 1, n))\n",
      "    print('done')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for number_of_nodes in range(50, 90, 10):\n",
      "    print(number_of_nodes)\n",
      "    generator_function = generic_hypergraph\n",
      "    hypergraph_properties = {\n",
      "        'number_of_nodes': number_of_nodes,\n",
      "        'edges_params': ((2, 20), (3, 30), (4, 20), (5, 15), (6, 10))\n",
      "    }\n",
      "    \n",
      "    print('Nodes models')\n",
      "    directory_name = 'hypergraph_nodes_%s' % number_of_nodes\n",
      "    execute_pipeline(generator_function, hypergraph_properties, directory_name, nodes_mapping)\n",
      "    \n",
      "    print('\\nEdges models')\n",
      "    directory_name = 'hypergraph_edges_%s' % number_of_nodes\n",
      "    execute_pipeline(generator_function, hypergraph_properties, directory_name, edges_mapping)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "50\n",
        "Nodes models\n",
        "1/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done\n",
        "\n",
        "Edges models\n",
        "1/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done\n",
        "60\n",
        "Nodes models\n",
        "1/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done\n",
        "\n",
        "Edges models\n",
        "1/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done\n",
        "70\n",
        "Nodes models\n",
        "1/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done\n",
        "\n",
        "Edges models\n",
        "1/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done\n",
        "80\n",
        "Nodes models\n",
        "1/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done\n",
        "\n",
        "Edges models\n",
        "1/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10/10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done\n"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# show serialized form of hypergraph (possible to recreate it later)\n",
      "!cat hypergraph_nodes_50/0_hypergraph.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[21, 46], [9, 5], [17, 50], [28, 37], [42, 36], [14, 7], [25, 18], [27, 21], [40, 11], [28, 23], [18, 37], [42, 35], [18, 34], [19, 12], [22, 39], [10, 20], [10, 27], [18, 50], [20, 14], [2, 26], [2, 21, 14], [1, 34, 14], [24, 26, 50], [16, 13, 45], [11, 12, 43], [49, 5, 31], [2, 34, 30], [2, 5, 14], [16, 41, 39], [49, 18, 34], [32, 48, 27], [26, 19, 23], [8, 25, 18], [16, 3, 43], [31, 38, 6], [32, 40, 18], [25, 20, 22], [16, 17, 19], [25, 3, 19], [8, 9, 23], [16, 27, 36], [41, 27, 15], [40, 2, 37], [41, 12, 45], [27, 43, 21], [32, 10, 21], [33, 20, 44], [18, 43, 15], [16, 49, 44], [40, 48, 3], [2, 44, 22, 38], [24, 15, 42, 39], [48, 42, 27, 22], [19, 28, 5, 30], [8, 29, 6, 30], [16, 26, 43, 7], [25, 2, 4, 23], [48, 3, 44, 14], [7, 10, 29, 39], [17, 5, 38, 31], [1, 37, 22, 15], [25, 20, 45, 7], [8, 50, 11, 21], [24, 9, 42, 13], [16, 9, 3, 15], [1, 26, 19, 7], [25, 41, 27, 45], [24, 32, 50, 45], [16, 41, 30, 7], [9, 12, 36, 30], [24, 25, 29, 38, 31], [16, 17, 32, 5, 6], [9, 34, 41, 50, 15], [16, 17, 25, 20, 5], [40, 10, 35, 50, 29], [40, 49, 11, 21, 45], [32, 41, 7, 13, 47], [15, 25, 2, 36, 39], [3, 11, 29, 35, 15], [6, 14, 18, 22, 23], [17, 42, 3, 36, 39], [32, 49, 35, 37, 38], [40, 17, 2, 11, 44], [8, 33, 50, 13, 22], [24, 48, 43, 21, 14], [49, 2, 24, 42, 14, 47], [2, 23, 8, 40, 27, 44], [47, 7, 25, 29, 14, 15], [17, 49, 36, 7, 12, 14], [2, 36, 5, 20, 28, 14], [1, 4, 5, 21, 28, 14], [48, 2, 34, 5, 23, 41], [48, 23, 10, 42, 26, 13], [33, 17, 19, 25, 42, 12], [33, 18, 50, 21, 24, 41]]"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This way I generated a lot of data for a pretty complex hypergraph. However, how do I analyze it?\n",
      "It would be nice to load data from disc and make some basic statistics.\n",
      "I know that atomistic is the most divergent, huh. However, rest should be fine.\n",
      "\n",
      "If I for example get to compare pykov (steady state distributions of Markov Chain based on transition matrix generated from the hypergraph) with model nodes, how big differences will be?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# read example of results with numpy\n",
      "pykov_results = np.loadtxt('hypergraph_nodes_50/0_pykov_nodes.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "analytical_results = np.loadtxt('hypergraph_nodes_50/0_analytical_nodes.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# compare arrays of results by computing their difference\n",
      "pykov_results - analytical_results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 45,
       "text": [
        "array([  4.92748203e-14,   1.01446629e-14,   3.62349040e-14,\n",
        "         4.63778321e-15,   2.31897834e-14,   4.92748203e-14,\n",
        "        -3.91284227e-14,  -2.60867716e-14,  -2.60867716e-14,\n",
        "        -2.60867716e-14,  -2.60867716e-14,  -2.60867716e-14,\n",
        "         1.15948917e-14,   1.01446629e-14,  -3.91284227e-14,\n",
        "        -1.44884105e-14,  -3.91284227e-14,   2.31897834e-14,\n",
        "         3.62349040e-14,   3.62349040e-14,   2.31897834e-14,\n",
        "         3.62349040e-14,  -1.45022883e-15,  -1.45022883e-15,\n",
        "         4.78228568e-14,  -2.60867716e-14,  -3.91284227e-14,\n",
        "         1.15948917e-14,  -2.60867716e-14,   1.15948917e-14,\n",
        "         4.92748203e-14,   3.62349040e-14,   4.92748203e-14,\n",
        "        -2.60867716e-14,   4.92748203e-14,   3.62349040e-14,\n",
        "         1.15948917e-14,   1.15948917e-14,  -2.60867716e-14,\n",
        "        -1.45022883e-15,  -3.91284227e-14,  -3.91284227e-14,\n",
        "        -2.60867716e-14,  -2.60867716e-14,  -2.60867716e-14,\n",
        "         2.31889161e-15,  -3.04443970e-15,   3.62349040e-14,\n",
        "         3.62349040e-14,  -3.91284227e-14])"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compare_results(base_directory, suffix_one, suffix_two):\n",
      "    \"\"\"Compute differences between two different methods for computing the same result\"\"\"\n",
      "    filenames = os.listdir(base_directory)\n",
      "    first_filenames = [filename for filename in filenames if filename.endswith(suffix_one)]\n",
      "    first_filenames.sort()\n",
      "    second_filenames = [filename for filename in filenames if filename.endswith(suffix_two)]\n",
      "    second_filenames.sort()\n",
      "    differences = []\n",
      "    for first, second in zip(first_filenames, second_filenames):\n",
      "        difference = np.loadtxt(base_directory + '/' + first) - np.loadtxt(base_directory + '/' + second)\n",
      "        differences.append(difference)\n",
      "    return differences"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# compare all the sets of sizes\n",
      "def compare_sets(base_directory_template, suffix_one, suffix_two):\n",
      "    directories = (base_directory_template % number_of_nodes for number_of_nodes in range(50, 90, 10))\n",
      "    for directory in directories:\n",
      "        print(directory)\n",
      "        differences = compare_results(directory, suffix_one, suffix_two)\n",
      "        print('average difference', np.average(np.abs(differences)))\n",
      "        print('variance of differences', np.var(differences))\n",
      "        print('-' * 80)\n",
      "        print()\n",
      "compare_sets('hypergraph_nodes_%s', 'pykov_nodes.csv', 'analytical_nodes.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "hypergraph_nodes_50\n",
        "average difference 2.48766320043e-14\n",
        "variance of differences 8.226043135e-28\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "hypergraph_nodes_60\n",
        "average difference 2.36902345252e-14\n",
        "variance of differences 7.83562261469e-28\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "hypergraph_nodes_70\n",
        "average difference 2.19816315447e-14\n",
        "variance of differences 6.67990066372e-28\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "hypergraph_nodes_80\n",
        "average difference 2.07090556726e-14\n",
        "variance of differences 6.26384928982e-28\n",
        "--------------------------------------------------------------------------------\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "compare_sets('hypergraph_nodes_%s', 'atomistic_nodes.csv', 'analytical_nodes.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "hypergraph_nodes_50\n",
        "average difference 0.000474616811594\n",
        "variance of differences 3.63764214325e-07\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "hypergraph_nodes_60\n",
        "average difference 0.000444607246377\n",
        "variance of differences 3.26531986978e-07\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "hypergraph_nodes_70\n",
        "average difference 0.000407904347826\n",
        "variance of differences 2.66943587062e-07\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "hypergraph_nodes_80\n",
        "average difference 0.000356900724638\n",
        "variance of differences 2.21134558129e-07\n",
        "--------------------------------------------------------------------------------\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "compare_sets('hypergraph_edges_%s', 'matrix_power_edges.csv', 'analytical_edges.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "hypergraph_edges_50\n",
        "average difference 8.44996189011e-11\n",
        "variance of differences 5.54932472914e-20\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "hypergraph_edges_60\n",
        "average difference 2.99177497468e-09\n",
        "variance of differences 1.80022375173e-16\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "hypergraph_edges_70\n",
        "average difference"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8.39051564135e-09\n",
        "variance of differences 5.36273893841e-16\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "hypergraph_edges_80\n",
        "average difference 3.22182446427e-06\n",
        "variance of differences 6.41941085925e-10\n",
        "--------------------------------------------------------------------------------\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Summing up:\n",
      "\n",
      "- differences are really small between pykov (steady states distributions of markov chain and analytical solutions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}